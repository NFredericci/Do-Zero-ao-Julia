---
title: "Como utilizar o XGBoost em Julia"
description: |
     Aprenda a utilizar o XGBoost em Julia, tanto para regressão quanto para classificação, incluindo etapas cruciais como separação de treino e teste, cálculo de métricas de erro e validação cruzada para ajuste dos hiperparâmetros.
categories:
  - Modelagem
  - Machine Learning
author:
  - name: Vitor Ribas Perrone
    affiliation: "Universidade Estadual de Campinas"
    url: https://github.com/VitorRibasP
    orcid: 0009-0009-6923-7712
date: "2025-10-28"
image: imagens/xgboost.png
lang: pt
format:
  html:
    toc: true
    toc-depth: 3
    self-contained: false
engine: knitr
draft: true
---

## Introdução

::: justify
Como uma evolução moderna dos métodos de boosting, o XGBoost utiliza árvores de decisão treinadas de forma iterativa para capturar padrões complexos nos dados. Seu desempenho elevado deve-se à combinação de paralelização, regularização e otimizações específicas de hardware e memória. Essas propriedades lhe conferem resultados superiores frente a diversos algoritmos tradicionais de machine learning.

Neste tutorial, você aprenderá a aplicar o XGBoost em Julia, incluindo a separação em conjuntos de treino e teste, a padronização das covariáveis, o cálculo de métricas de erro e a escolha do melhor conjunto de hiperparâmetros por meio de validação cruzada.
:::

## XGBoost para Regressão
:::justify
Primeiro, vamos instalar e carregar os pacotes necessários:
:::

```{julia}
Pkg.add("MLJ")
using MLJ
Pkg.add("XGBoost") #apenas instalar
Pkg.add("MLJXGBoostInterface")
using MLJXGBoostInterface
Pkg.add("Random")
using Random
Pkg.add("Distributions")
using Distributions
Pkg.add("DataFrames")
using DataFrames
```
::: justify
Para um XGBoost do tipo regressão, a variável resposta deve ser numérica. Vamos criar o vetor `Y` com a variável resposta e o DataFrame `X` com as covariáveis:
:::
```{julia}
Random.seed!(123)
X1 = rand(Normal(20, 1), 100)
X2 = rand(Normal(0, 1), 100)
X3 = rand(Normal(-5, 1), 100)
Y = X1 .+ X2.*X3
X = DataFrame(X1 = X1, X2 = X2, X3 = X3)
```
::: justify
Agora, vamos seprar os dados em treino e teste utilizando a função `partition`. No caso, vamos deixar 80% dos dados para treino e os demais para teste.
:::
```{julia}
Random.seed!(123)
treino, teste = partition(eachindex(Y), 0.8, shuffle=true)
```
::: justify
Para XGBoost do tipo regressão, utilizamos `XGBoostRegressor`, que possui como alguns dos seus parâmetros:

- `eta`: taxa de aprendizado;
- `num_round`: número de iterações, ou número de árvores;
- `max_depth`: profundidade máxima das árvores;
- `gamma`: ganho mínimo na redução do erro para criação de um nó;
- `subsample`: proporção de observações utilizadas em cada árvore;
- `colsample_bytree`: proporção de covariáveis utilizadas em cada árvore.
:::
```{julia}
xgboost_regressao = XGBoostRegressor(num_round = 50,
                                     max_depth = 3,
                                     eta = 0.1)
```
::: justify
Dessa forma, podemos utilizar as funções `machine` e `fit` para o ajuste do modelo.
:::
```{julia}
machine_xgboost_regressao = machine(xgboost_regressao, X, Y)
fit!(machine_xgboost_regressao, rows=treino)
```
::: justify
Com o modelo ajustado, podemos fazer predições nos conjuntos de dados de treino e teste utilizando a função `predict`.
:::
```{julia}
predict(machine_xgboost_regressao, rows=treino)
predict(machine_xgboost_regressao, rows=teste)
```
::: justify
Também é possível computar diretamente alguma métrica de erro do modelo. Por exemplo, para computar a raiz quadrada do erro quadrático médio (RMSE), utilizamos a função `rms`.
:::
```{julia}
rms(Y[treino], predict(machine_xgboost_regressao, rows=treino))
rms(Y[teste], predict(machine_xgboost_regressao, rows=teste))
```
## XGBoost para Classificação
::: justify
Também podemos utilizar a floresta aleatória para classificação, isto é, quando nossa variável resposta é categórica. Para isso, vamos criar um vetor `Y` contendo as categorias e um DataFrame `X` contendo as covariáveis. É interessante ressaltar que a função `categorical` especifica o tipo do vetor da variável resposta como categórico, que é exigido para o ajuste do modelo.
:::
```{julia}
Random.seed!(123)
X1 = vcat([rand(Normal(mu, 1), 50) for mu in [2, 2, 17, 15]]...)
X2 = vcat([rand(Normal(mu, 1), 50) for mu in [3, 4, 14, 16]]...)
X3 = vcat([rand(Normal(mu, 1), 50) for mu in [3, 1, 13, 15]]...)
Y = categorical(repeat(["A", "B", "C", "D"], inner=50))
X = DataFrame(X1 = X1, X2 = X2, X3 = X3)
```
::: justify
Assim como no caso anterior, podemos dividir em treino e teste utilizando a função `partition`.
:::
```{julia}
Random.seed!(123)
treino, teste = partition(eachindex(Y), 0.8, shuffle=true)
```
::: justify
No caso anterior, o interesse era em predizer uma variável numérica. Como agora o interesse é em realizar classificações, a função referente ao modelo passa a ser `XGBoostClassifier`, em que os demais parâmetros são os mesmos.
:::
```{julia}
xgboost_classificacao = XGBoostClassifier(num_round = 50,
                                     max_depth = 3,
                                     eta = 0.1)
```
::: justify
Com isso, ajustamos o modelo utilizando as funções `machine` e `fit`.
:::
```{julia}
machine_xgboost_classificacao = machine(xgboost_classificacao, X, Y)
fit!(machine_xgboost_classificacao, rows=treino)
```
::: justify
Para fazer predições nos conjuntos de treino e teste, também utilizamos a função `predict`. Entretanto, como se tratam de classificações, as predições retornam vetores contendo a probabilidade de cada observação pertencer a cada uma das categorias.
:::
```{julia}
predict(machine_xgboost_classificacao, rows=treino)
predict(machine_xgboost_classificacao, rows=teste)
```
::: justify
Para extrair a classificação das predições ao invés das probabilidades, utilizamos `mode.()`, em que as predições são inseridas dentro dos parênteses. 
:::
```{julia}
mode.(predict(machine_xgboost_classificacao, rows=treino))
mode.(predict(machine_xgboost_classificacao, rows=teste))
```
::: justify
Também podemos calcular métricas de erro para o nosso modelo, por exemplo, podemos calcular a acurácia utilizando a função `accuracy`.
:::
```{julia}
accuracy(mode.(predict(machine_xgboost_classificacao, rows=treino)), Y[treino])
accuracy(mode.(predict(machine_xgboost_classificacao, rows=teste)), Y[teste])
```
::: justify
Uma outra maneira interessante de visualizar os resultados do modelo é por meio de uma matriz de confusão, e podemos elaborar uma utilizando a função `confusion_matrix`. Dessa forma, a matriz de confusão para o banco de dados de treino é dada por:
:::
```{julia}
confusion_matrix(Y[treino], mode.(predict(machine_xgboost_classificacao, rows=treino)))
```
::: justify
E a matriz de confusão para o banco de dados de teste é dada por:
:::
```{julia}
confusion_matrix(Y[teste], mode.(predict(machine_xgboost_classificacao, rows=teste)))
```

## Ajuste dos Hiperparâmetros

::: justify
Ao trabalhar com modelos de aprendizado de máquina, é de interesse saber o valor ideal dos nossos hiperparâmetros em questão. No caso do XGBoost existem diversos parâmetros que podemos especificar. Felizmente, existe uma maneira mutio eficaz de comparar diversas combinações de hiperparâmetros e escolher a melhor por meio de validação cruzada.

Para realizar esse procedimento de escolher a melhor combinação de hiperparâmetros por meio de validação cruzada e grid search, é necessário utilizar a função `TunedModel`, que possui como argumentos:

- `model`: modelo a ser ajustado;
- `resampling`: técnica de divisão dos dados;
- `range`: valores para cada parâmetro a serem testados;
- `measure`: métrica de erro a ser considerada para escolha do melhor valor.

Com isso, para especificar os valores de cada hiperparâmetro a serem testados, é necessário utilizar a função `range`, determinando os valores mínimo e o máximo a serem considerados. Já para utilizar validação cruzada do tipo k-fold, é necessário utilizar a função `CV`, especificando por meio do argumento `nfolds` o número de dobras a serem consideradas. Para o exemplo, vou utilizar 5 folds e a acurácia por se tratar de classificações. 
:::
```{julia}
Random.seed!(123)

valores_eta = range(xgboost_classificacao, :eta, lower = 0.1, upper = 0.5)
valores_max_depth = range(xgboost_classificacao, :max_depth, lower = 2, upper = 3)
xgboost_escolher_k = TunedModel(model=xgboost_classificacao,
                               resampling=CV(nfolds=5),
                               range=[valores_eta, valores_eta],
                               measure=accuracy)
```
::: justify
Para ajustar o modelo, novamente utilizamos as funções `machine` e `fit`.
:::
```{julia}
machine_escolher_k = machine(xgboost_escolher_k, X, Y)
fit!(machine_escolher_k)
```
::: justify
Agora, para extrair o melhor modelo, utilizamos a função `fitted_params` e especificamos o melhor modelo com `.best_model`. No caso, o melhor modelo foi aquele com taxa de aprendizado 0.39 e profundidade máxima igual a 3. 
:::
```{julia}
melhor_xgboost = fitted_params(machine_escolher_k).best_model
```

## Conclusão

::: justify
Neste tutorial, mostramos como implementar o XGBoost em Julia para regressão e classificação, enfatizando etapas cruciais como divisão dos dados em treino e teste, avaliação de métricas de erro, aplicação de predições e escolha do conjunto ideal de hiperparâmetros por validação cruzada.

Apesar de sua eficácia, o XGBoost pode apresentar sensibilidade a escolhas inadequadas de hiperparâmetros e configurações. Elementos como taxa de aprendizado, número de iterações e parâmetros de regularização impactam diretamente o desempenho final. Dessa forma, processos criteriosos de experimentação e comparação são recomendados para maximizar o potencial do modelo, processo que é facilitado pela alta eficiência computacional de Julia.
:::

::: callout-note
Ferramentas de IA foram utilizadas para correção ortográfica e aprimoramento do texto.
:::
