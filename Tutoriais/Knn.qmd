---
title: "Como utilizar o k-NN em Julia"
description: |
     Aprenda a utilizar o k-NN em Julia, tanto para regressão quanto para classificação, incluindo etapas cruciais como separação de treino e teste, cálculo de métricas de erro e validação cruzada para escolha do melhor número de vizinhos.
categories:
  - Modelagem
  - Machine Learning
author:
  - name: Vitor Ribas Perrone
    affiliation: "Universidade Estadual de Campinas"
    url: https://github.com/VitorRibasP
    orcid: 0009-0009-6923-7712
date: "2025-10-28"
image: imagens/knn.png
lang: pt
format:
  html:
    toc: true
    toc-depth: 3
    self-contained: false
engine: knitr
draft: true
---

## Introdução

::: justify
O k-NN (k-Nearest Neighbors) é um algoritmo simples, porém poderoso, amplamente utilizado em aprendizado de máquina para tarefas de regressão e classificação. Ele se baseia na ideia de que observações próximas no espaço das covariáveis tendem a ter respostas semelhantes. Por isso, o modelo não faz suposições paramétricas sobre a forma da relação entre variáveis. Apesar de sua simplicidade, o k-NN pode ser muito eficaz em diversos cenários.

Neste tutorial, você aprenderá a aplicar o k-NN em Julia, incluindo a separação em conjuntos de treino e teste, a padronização das covariáveis, o cálculo de métricas de erro e a escolha do melhor valor de `K` por validação cruzada.
:::

## k-NN para Regressão
:::justify
Primeiro, vamos instalar e carregar os pacotes necessários:
:::
```{julia}
Pkg.add("MLJ")
using MLJ
Pkg.add("NearestNeighborModels")
using NearestNeighborModels
Pkg.add("Random")
using Random
Pkg.add("Distributions")
using Distributions
Pkg.add("DataFrames")
using DataFrames
```
::: justify
Para um k-KK do tipo regressão, a variável resposta deve ser numérica. Vamos criar o vetor `Y` com a variável resposta e o DataFrame `X` com as covariáveis:
:::
```{julia}
Random.seed!(123)
X1 = rand(Normal(25, 1), 100)
X2 = rand(Normal(0, 1), 100)
X3 = rand(Normal(5, 1), 100)
Y = X1 .+ X2.*X3
X = DataFrame(X1 = X1, X2 = X2, X3 = X3)
```
::: justify
Agora, vamos seprar os dados em treino e teste utilizando a função `partition`. No caso, vamos deixar 80% dos dados para treino e os demais para teste.
:::
```{julia}
Random.seed!(123)
treino, teste = partition(eachindex(Y), 0.8, shuffle=true)
```
::: justify
Para regressão, utilizamos `KNNRegressor` e especificamos o número de vizinhos por meio do argumento `K`:
:::
```{julia}
knn_regressao = KNNRegressor(K = 5)
```
::: justify
Entretanto, não basta apenas ajustar o modelo. Como ele é baseado na norma euclidiana, é necessário padronizar as covariáveis, a fim de escala dos dados não influenciar na eficácia do modelo. Para isso, vamos utilizar a função `Pipeline` para espeficiar um pipeline que padroniza os dados utilizando `Standardizer` e após isso ajusta o modelo.  
:::
```{julia}
pipeline_knn_regressao = Pipeline(
    standardizer = Standardizer(),
    modelo = knn_regressao)
```
::: justify
Por fim, resta apenas utilizar as funções `machine` e `fit` a fim de ajustar o modelo em questão, no caso, um k-NN do tipo regressão com 5 vizinhos.
:::
```{julia}
machine_knn_regressao = machine(pipeline_knn_regressao, X, Y)
fit!(machine_knn_regressao, rows=treino)
```
::: justify
Com o modelo ajustado, podemos fazer predições nos conjuntos de dados de treino e teste utilizando a função `predict`.
:::
```{julia}
predict(machine_knn_regressao, rows=treino)
predict(machine_knn_regressao, rows=teste)
```
::: justify
Também é possível computar diretamente alguma métrica de erro do modelo. Por exemplo, para computar a raiz quadrada do erro quadrático médio (RMSE), utilizamos a função `rms`.
:::
```{julia}
rms(Y[treino], predict(machine_knn_regressao, rows=treino))
rms(Y[teste], predict(machine_knn_regressao, rows=teste))
```
## k-NN para Classificação
::: justify
Também podemos utilizar o k-NN para classificação, isto é, quando nossa variável resposta é categórica. Para isso, vamos criar um vetor `Y` contendo as categorias e um DataFrame `X` contendo as covariáveis. É interessante ressaltar que a função `categorical` especifica o tipo do vetor da variável resposta como categórico, que é exigido para o ajuste do modelo.
:::
```{julia}
Random.seed!(123)
X1 = vcat([rand(Normal(mu, 1), 50) for mu in [2, 2, 17, 15]]...)
X2 = vcat([rand(Normal(mu, 1), 50) for mu in [3, 4, 14, 16]]...)
X3 = vcat([rand(Normal(mu, 1), 50) for mu in [3, 1, 13, 15]]...)
Y = categorical(repeat(["A", "B", "C", "D"], inner=50))
X = DataFrame(X1 = X1, X2 = X2, X3 = X3)
```
::: justify
Assim como no caso anterior, podemos dividir em treino e teste utilizando a função `partition`.
:::
```{julia}
Random.seed!(123)
treino, teste = partition(eachindex(Y), 0.8, shuffle=true)
```
::: justify
No caso anterior, o interesse era em predizer uma variável numérica. Como agora o interesse é em realizar classificações, a função referente ao modelo passa a ser `KNNClassifier`, em que `K` ainda é a quantidade de vizinhos a ser utilizada.
:::
```{julia}
knn_classificacao = KNNClassifier(K = 5)
```
::: justify
Assim como no modelo anterior, é necessário padronizar as covariáveis, e criamos um pipeline da mesma forma.
:::
```{julia}
pipeline_knn_classificacao = Pipeline(
    standardizer = Standardizer(),
    modelo = knn_classificacao)
```
::: justify
Com isso, ajustamos o modelo utilizando as funções `machine` e `fit`.
:::
```{julia}
machine_knn_classificacao = machine(pipeline_knn_classificacao, X, Y)
fit!(machine_knn_classificacao, rows=treino)
```
::: justify
Para fazer predições nos conjuntos de treino e teste, também utilizamos a função `predict`. Entretanto, como se tratam de classificações, as predições retornam vetores contendo a probabilidade de cada observação pertencer a cada uma das categorias.
:::
```{julia}
predict(machine_knn_classificacao, rows=treino)
predict(machine_knn_classificacao, rows=teste)
```
::: justify
Para extrair a classificação das predições ao invés das probabilidades, utilizamos `mode.()`, em que as predições são inseridas dentro dos parênteses. 
:::
```{julia}
mode.(predict(machine_knn_classificacao, rows=treino))
mode.(predict(machine_knn_classificacao, rows=teste))
```
::: justify
Também podemos calcular métricas de erro para o nosso modelo, por exemplo, podemos calcular a acurácia utilizando a função `accuracy`.
:::
```{julia}
accuracy(mode.(predict(machine_knn_classificacao, rows=treino)), Y[treino])
accuracy(mode.(predict(machine_knn_classificacao, rows=teste)), Y[teste])
```
::: justify
Uma outra maneira interessante de visualizar os resultados do modelo é por meio de uma matriz de confusão, e podemos elaborar uma utilizando a função `confusion_matrix`. Dessa forma, a matriz de confusão para o banco de dados de treino é dada por:
:::
```{julia}
confusion_matrix(Y[treino], mode.(predict(machine_knn_classificacao, rows=treino)))
```
::: justify
E a matriz de confusão para o banco de dados de teste é dada por:
:::
```{julia}
confusion_matrix(Y[teste], mode.(predict(machine_knn_classificacao, rows=teste)))
```

## Ajuste do Número de Vizinhos
::: justify
Ao trabalhar com modelos de aprendizado de máquina, é de interesse saber o valor ideal dos nossos hiperparâmetros em questão. No caso do k-NN, é necessário determinar qual o valor ideal de `K`. Felizmente, existe uma maneira mutio eficaz de comparar diversos valores de `K` e escolher o melhor por meio de validação cruzada.

Para realizar esse procedimento de escolher o melhor valor do `K` por meio de validação cruzada e grid search, é necessário utilizar a função `TunedModel`, que possui como argumentos:

- `model`: modelo a ser ajustado;
- `resampling`: técnica de divisão dos dados;
- `range`: valores de `K` a serem avaliados;
- `measure`: métrica de erro a ser considerada para escolha do melhor valor.

Com isso, para especificar os valores de `K` a serem testados, é necessário utilizar a função `range`, determinando os valores mínimo e o máximo a serem considerados. Já para utilizar validação cruzada do tipo k-fold, é necessário utilizar a função `CV`, especificando em `nfolds` o número de dobras a serem consideradas. Para o exemplo, vou utilizar 5 folds e a acurácia por se tratar de classificações. 
:::
```{julia}
Random.seed!(123)
valores_k = range(knn_classificacao, :K, lower=1, upper=10)
knn_escolher_k = TunedModel(model=knn_classificacao,
                               resampling=CV(nfolds=5),
                               range=valores_k,
                               measure=accuracy)

```
::: justify
Assim como no modelo anterior, é necessário padronizar as covariáveis, e criamos um pipeline da mesma forma.
:::

```{julia}
pipeline_melhor_k = Pipeline(
    standardizer = Standardizer(),
    modelo = knn_escolher_k)
```

::: justify
Para ajustar o modelo, novamente utilizamos as funções `machine` e `fit`.
:::
```{julia}
machine_escolher_k = machine(pipeline_melhor_k, X, Y)
fit!(machine_escolher_k)
```
::: justify
Agora, para extrair o melhor modelo, utilizamos a função `fitted_params` e especificamos o melhor modelo com `.best_model`. No caso, o melhor modelo variando o número de vizinhos de 1 a 10 foi aquele com 4 vizinhos. 
:::
```{julia}
melhor_knn = fitted_params(machine_escolher_k).best_model
```

## Conclusão

::: justify
O k-NN é um método intuitivo e flexível, adequado para diversas tarefas de aprendizado supervisionado. Ele não exige modelagem explícita da relação entre covariáveis e respostas, o que o torna ideal para situações em que a estrutura dos dados é complexa ou desconhecida.

Neste tutorial, mostramos como implementar o k-NN em Julia para regressão e classificação, enfatizando etapas cruciais como padronização das covariáveis, avaliação de métricas de erro, interpretação de predições e escolha do número ideal de vizinhos por validação cruzada.
:::

::: callout-note
Ferramentas de IA foram utilizadas para correção ortográfica e aprimoramento do texto.
:::
